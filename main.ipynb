{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio openai langchain langchain-community youtube_transcript_api tiktoken transformers langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Summarizer by Case Done\n",
    "- This app will get YouTube info and transcript, and allow you to summarize it.\n",
    "- It is based on LangChain map-reduce method powered by Llama 3.2 via Ollama.\n",
    "- Start by providing a valid YouTube URL in the textbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pisek/miniconda3/envs/py3p12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import html\n",
    "import gradio as gr\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_TEMPLATE_TXT = \"\"\"Write a detail summary of this text section in bullet points. \n",
    "Use '-' for bullet points and answer only the bullet points.\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "    \n",
    "COMBINE_TEMPLATE_TXT = \"\"\"Combine these summaries into a final summary in bullet points.\n",
    "Use '-' for bullet points and answer only the bullet points.\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "FINAL SUMMARY:\"\"\"\n",
    "\n",
    "map_prompt_txt = MAP_TEMPLATE_TXT\n",
    "combine_prompt_txt = COMBINE_TEMPLATE_TXT\n",
    "\n",
    "\n",
    "QUESTION_TEMPLATE_TXT = \"\"\"Write a detailed summary (in bullet points, using \"-\" for bullets) of the following:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "REFINE_TEMPLATE_TXT = \"\"\"Your job is to produce a final summary in bullet points (using \"-\" for bullets).\n",
    "You are provided an existing summary here:\n",
    "<existing_summary>\n",
    "{existing_answer}\n",
    "</existing_summary>\n",
    "\n",
    "You are provided new text.\n",
    "<new_text>\n",
    "{text}\n",
    "</new_text>\n",
    "\n",
    "Given the new text, refine the original summary.\n",
    "If the context isn't useful, return the original summary. Answer your summary only, not other texts.\n",
    "\"\"\"\n",
    "\n",
    "refine_prompt_txt = REFINE_TEMPLATE_TXT\n",
    "question_prompt_txt = QUESTION_TEMPLATE_TXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"sample-text.txt\", \"r\") as f:\n",
    "#     transcript = f.read()\n",
    "\n",
    "model = \"llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "\n",
    "chunk_size = 2000 # this is in tokens\n",
    "overlap_size = 0 # this is in tokens\n",
    "temperature = 0.5\n",
    "\n",
    "mapreduce_num_predict = 512\n",
    "map_num_predict = 512 # number of tokens to predict, Default: 128, -1 = infinite generation, -2 = fill context\n",
    "combine_num_predict = 2048\n",
    "refine_num_predict = 2048\n",
    "\n",
    "global config\n",
    "config = {}\n",
    "config[\"model\"] = model\n",
    "config[\"base_url\"] = base_url\n",
    "config[\"chunk_size\"] = chunk_size\n",
    "config[\"overlap_size\"] = overlap_size\n",
    "config[\"temperature\"] = temperature\n",
    "config[\"mapreduce_num_predict\"] = mapreduce_num_predict\n",
    "config[\"map_num_predict\"] = map_num_predict\n",
    "config[\"combine_num_predict\"] = combine_num_predict\n",
    "config[\"refine_num_predict\"] = refine_num_predict\n",
    "\n",
    "global text_to_summarize\n",
    "text_to_summarize = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_info(url: str):\n",
    "    \"\"\"Get video title and description.\"\"\"\n",
    "    # try:\n",
    "    video_id = extract_video_id(url)\n",
    "    if not video_id:\n",
    "        raise ValueError(\"Invalid YouTube URL\")\n",
    "        \n",
    "    # Get video page content\n",
    "    video_url = f\"https://youtube.com/watch?v={video_id}\"\n",
    "    content = urlopen(video_url).read().decode('utf-8')\n",
    "    \n",
    "    # Extract title\n",
    "    title_match = re.search(r'\"title\":\"([^\"]+)\"', content)\n",
    "    title = html.unescape(title_match.group(1)) if title_match else \"Unknown Title\"\n",
    "    \n",
    "    # Extract description\n",
    "    desc_match = re.search(r'\"description\":{\"simpleText\":\"([^\"]+)\"', content)\n",
    "    description = html.unescape(desc_match.group(1)) if desc_match else \"No description available\"\n",
    "    \n",
    "    return title, description\n",
    "    # except Exception as e:\n",
    "    #     return {\"title\": \"Error\", \"description\": str(e)}\n",
    "    \n",
    "    \n",
    "def extract_video_id(url):\n",
    "    \"\"\"Extract YouTube video ID from URL.\"\"\"\n",
    "    patterns = [\n",
    "        r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
    "        r'(?:embed\\/)([0-9A-Za-z_-]{11})',\n",
    "        r'(?:youtu\\.be\\/)([0-9A-Za-z_-]{11})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_text_splitter(chunk_size: int, overlap_size: int):\n",
    "    return RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=overlap_size)\n",
    "\n",
    "def convert_text_to_tokens(text, encoder=\"gpt-3.5-turbo\"):\n",
    "    enc = tiktoken.encoding_for_model(encoder)\n",
    "    return enc.encode(text)\n",
    "\n",
    "def get_larger_context_size(token_count):\n",
    "    num_ctxes = [1024*i for i in range(1, 100)]\n",
    "    num_ctx = next(ctx for ctx in num_ctxes if ctx > token_count) # pick the first context size that is greater than the token counts\n",
    "    return num_ctx\n",
    "\n",
    "\n",
    "def get_youtube_transcript(url):\n",
    "    \"\"\"\n",
    "    Extract transcript from a YouTube video URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): YouTube video URL\n",
    "        \n",
    "    Returns:\n",
    "        str: Full transcript text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract video ID from URL\n",
    "        video_id = extract_video_id(url)\n",
    "        if not video_id:\n",
    "            raise ValueError(\"Invalid YouTube URL\")\n",
    "            \n",
    "        # Get transcript\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        \n",
    "        # Combine transcript pieces\n",
    "        full_transcript = ' '.join(entry['text'] for entry in transcript_list)\n",
    "\n",
    "        enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        count = len(enc.encode(full_transcript))\n",
    "        \n",
    "        return full_transcript, count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0\n",
    "    \n",
    "    \n",
    "def get_llm(model: str, base_url: str, temperature: float, num_ctx: int=2048, num_predict: int=256):\n",
    "    llm = ChatOllama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=num_predict\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "def convert_text_to_split_docs(text, chunk_size, overlap_size):\n",
    "    docs = [Document(\n",
    "        page_content=text,\n",
    "        # metadata={\"source\": url}\n",
    "    )]\n",
    "    text_splitter = get_text_splitter(chunk_size=chunk_size, overlap_size=overlap_size)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    return split_docs\n",
    "    \n",
    "    \n",
    "def get_summary_map_reduce_langchain(text_to_summarize: str, map_prompt_txt: str, combine_prompt_text:str):\n",
    "\n",
    "    global config\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    overlap_size = config[\"overlap_size\"]\n",
    "    model = config[\"model\"]\n",
    "    base_url = config[\"base_url\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "    mapreduce_num_predict = config[\"mapreduce_num_predict\"]\n",
    "    \n",
    "    # transcript, tokencount = get_youtube_transcript(url)\n",
    "    split_docs = convert_text_to_split_docs(text_to_summarize, chunk_size, overlap_size)\n",
    "    \n",
    "    tokens = (mapreduce_num_predict+chunk_size)\n",
    "    num_ctx = get_larger_context_size(tokens)\n",
    "    llm = get_llm(model, base_url, temperature, num_predict=mapreduce_num_predict, num_ctx=num_ctx, verbose=True)\n",
    "\n",
    "    map_prompt = PromptTemplate(\n",
    "        template=map_prompt_txt,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "\n",
    "    combine_prompt = PromptTemplate(\n",
    "        template=combine_prompt_text,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    chain = load_summarize_chain(llm, \n",
    "                                 chain_type=\"map_reduce\",\n",
    "                                 map_prompt=map_prompt,\n",
    "                                 combine_prompt=combine_prompt,\n",
    "                                 verbose=True\n",
    "                                 )\n",
    "    \n",
    "    output = chain.invoke(split_docs)\n",
    "    \n",
    "    return output['output_text']\n",
    "\n",
    "def get_summary_map_reduce_manual(text_to_summarize: str, map_prompt_txt: str, combine_prompt_text:str):\n",
    "\n",
    "    global config\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    overlap_size = config[\"overlap_size\"]\n",
    "    model = config[\"model\"]\n",
    "    base_url = config[\"base_url\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "    map_num_predict = config[\"map_num_predict\"]\n",
    "    combine_num_predict = config[\"combine_num_predict\"]\n",
    "    \n",
    "    split_docs = convert_text_to_split_docs(text_to_summarize, chunk_size, overlap_size)\n",
    "    \n",
    "    map_prompt = PromptTemplate(\n",
    "        template=map_prompt_txt,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "\n",
    "    combine_prompt = PromptTemplate(\n",
    "        template=combine_prompt_text,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "\n",
    "    map_num_ctx = get_larger_context_size(map_num_predict+chunk_size)\n",
    "    \n",
    "    llm_map = ChatOllama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=map_num_ctx,\n",
    "        num_predict=map_num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    summaries = []\n",
    "\n",
    "    for i, splic_doc in enumerate(tqdm(split_docs, desc=\"Mapping...\")):\n",
    "        full_prompt = map_prompt.format_prompt(text=splic_doc.page_content)\n",
    "        output = llm_map.invoke(full_prompt.text)\n",
    "        summaries.append(output.content)\n",
    "    \n",
    "    combined_summaries = \"\\n\".join(summaries)\n",
    "\n",
    "    full_prompt = combine_prompt.format_prompt(text=combined_summaries)\n",
    "\n",
    "    token_counts = len(convert_text_to_tokens(full_prompt.text))\n",
    "\n",
    "    combine_num_ctx = get_larger_context_size(token_counts+combine_num_predict)\n",
    "\n",
    "    llm_combine = ChatOllama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=combine_num_ctx,\n",
    "        num_predict=combine_num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    output_comb = llm_combine.invoke(full_prompt.text)\n",
    "\n",
    "    return output_comb.content\n",
    "\n",
    "def get_summary_refine_langchain(text_to_summarize: str, refine_prompt_txt: str, question_prompt_text:str):\n",
    "\n",
    "    global config\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    overlap_size = config[\"overlap_size\"]\n",
    "    model = config[\"model\"]\n",
    "    base_url = config[\"base_url\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "    refine_num_predict = config[\"refine_num_predict\"]\n",
    "    \n",
    "    split_docs = convert_text_to_split_docs(text_to_summarize, chunk_size, overlap_size)\n",
    "    \n",
    "    refine_prompt = PromptTemplate.from_template(\n",
    "        template=refine_prompt_txt,\n",
    "        # input_variables=[\"text\", \"existing_answer\"]\n",
    "    )\n",
    "\n",
    "    question_prompt = PromptTemplate.from_template(\n",
    "        template=question_prompt_text,\n",
    "        # input_variables=[\"text\"]\n",
    "    )\n",
    "\n",
    "    num_ctx = get_larger_context_size(int(2*(refine_num_predict+chunk_size)))\n",
    "    \n",
    "    llm = ChatOllama(\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature,\n",
    "        num_ctx=num_ctx,\n",
    "        num_predict=refine_num_predict,\n",
    "        format='',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    chain = load_summarize_chain(\n",
    "        llm, \n",
    "        chain_type=\"refine\",\n",
    "        question_prompt=question_prompt,\n",
    "        refine_prompt=refine_prompt,\n",
    "        document_variable_name=\"text\", \n",
    "        initial_response_name=\"existing_answer\",\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    output_comb = chain.invoke(split_docs)\n",
    "\n",
    "    return output_comb['output_text']\n",
    "\n",
    "def get_summary():\n",
    "    return \"yo yo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging\n",
    "with open(\"sample-text.txt\", \"r\") as f:\n",
    "    text_to_summarize = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Mapping...: 100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a detailed summary (in bullet points, using \"-\" for bullets) of the following:\n",
      "\n",
      "\"think it's possible that physics has exploits and we should be trying to find them arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow somehow gives you a rounding error in the floating Point synthetic intelligences are kind of like the next stage of development and I don't know where it leads to like at some point I suspect the universe is some kind of a puzzle these synthetic AIS will uncover that puzzle and solve it the following is a conversation with Andre capothy previously the director of AI at Tesla and before that at open Ai and Stanford he is one of the greatest scientists engineers and Educators in the history of artificial intelligence this is the Lex Friedman podcast to support it please check out our sponsors and now dear friends here's Andre capathi what is a neural network and why does it seem to uh do such a surprisingly good job of learning what is a neural network it's a mathematical abstraction of the brain I would say that's how it was originally developed at the end of the day it's a mathematical expression and it's a fairly simple mathematical expression when you get down to it it's basically a sequence of Matrix multiplies which are really dot products mathematically and some nonlinearities thrown in and so it's a very simple mathematical expression and it's got knobs in it many knobs many knobs and these knobs are Loosely related to basically the synapses in your brain they're trainable they're modifiable and so the idea is like we need to find the setting of The Knobs that makes the neural nut do whatever you want it to do like classify images and so on and so there's not too much mystery I would say in it like um you might think that basically don't want to endow it with too much meaning with respect to the brain and how it works it's really just a complicated mathematical expression with knobs and those knobs need a proper setting for it to do something uh desirable yeah but poetry is just the c\"\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# YouTube Summarizer by Case Done\n",
    "- This app will get YouTube info and transcript, and allow you to summarize it.\n",
    "- It is based on LangChain map-reduce method powered by Llama 3.2 via Ollama.\n",
    "- Start by providing a valid YouTube URL in the textbox.\n",
    "                \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            pass\n",
    "        with gr.Column(scale=1, min_width=25):\n",
    "            bttn_clear = gr.ClearButton(interactive=True, variant='stop')\n",
    "    \n",
    "    with gr.Tab(label=\"YouTube\") as tab1:\n",
    "        \n",
    "        gr.Markdown(\"## Input YouTube Link Here:\")\n",
    "        url = gr.Textbox(label='YouTube URL', value=\"https://youtu.be/bvPDQ4-0LAQ\")\n",
    "        \n",
    "        gr.Markdown(\"## YouTube Info\")\n",
    "        with gr.Row(equal_height=False):\n",
    "            with gr.Column(scale=4):\n",
    "                with gr.Accordion(\"YouTube Information\"):\n",
    "                    title = gr.Textbox(label='Title', lines=2, max_lines=5, show_copy_button=True)\n",
    "                    desc = gr.Textbox(label='Description', lines=10, max_lines=20, \n",
    "                                      autoscroll=False, show_copy_button=True)\n",
    "            with gr.Column(scale=1, min_width=25):\n",
    "                bttn_info_get = gr.Button('Get Info', variant='primary', )\n",
    "        \n",
    "        gr.Markdown(\"## Transcript\")\n",
    "        with gr.Row(equal_height=False):              \n",
    "            with gr.Column(scale=4):\n",
    "                trns_raw = gr.Textbox(label='Transcript', show_copy_button=True, autoscroll=True,\n",
    "                                      lines=10, max_lines=500,\n",
    "                                      value=text_to_summarize[:2000],\n",
    "                                      interactive=True)\n",
    "            with gr.Column(scale=1, min_width=25):\n",
    "                bttn_trns_get = gr.Button(\"Get Transcript\", variant='primary')\n",
    "                tkncount = gr.Number(label='Token Count (~)', interactive=False)\n",
    "        \n",
    "    with gr.Tab(label=\"Summarize\") as tab2:\n",
    "        gr.Markdown(\"## Model Parameters\")\n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=100):\n",
    "                    model = gr.Dropdown(choices=['llama3.2'], value='llama3.2', label='Ollama models', interactive=True)\n",
    "                with gr.Column(scale=1, min_width=100):\n",
    "                    base_url = gr.Textbox(label='Base URL', value='http://localhost:11434', interactive=True)\n",
    "                with gr.Column(scale=1, min_width=100):\n",
    "                    temperature = gr.Number(label='Temperature', minimum=0.0, step=0.01, precision=-2)\n",
    "\n",
    "        gr.Markdown(\"## Text Splitting Parameters\")\n",
    "        # with gr.Accordion(label='Text Splitting Parameters', open=False):\n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=100):\n",
    "                    chunk = gr.Number(label='Chunk Size', minimum=200, step=100, value=2000)\n",
    "                with gr.Column(scale=1, min_width=100):\n",
    "                    overlap = gr.Number(label='Overlap Size', minimum=0, step=10, value=0)\n",
    "\n",
    "        gr.Markdown(\"## Approaches\")\n",
    "        # with gr.Tabs() as tabs:\n",
    "        with gr.Tab(label=\"Map-Reduce LangChain\") as tab_mrlc:\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=25):\n",
    "                    mapreduce_num_predict = gr.Number(label='Number of tokens to predict', minimum=128, step=128, value=config[\"mapreduce_num_predict\"], interactive=True)\n",
    "                with gr.Column(scale=4):\n",
    "                    with gr.Accordion(label=\"Prompt Templates\", open=False):\n",
    "                        map_prompt_txt_mrlc = gr.Textbox(label=\"Prompt for the mapping step\", value=MAP_TEMPLATE_TXT,\n",
    "                                                        lines=10, max_lines=50, show_copy_button=True, interactive=True)\n",
    "                        combine_prompt_txt_mrlc = gr.Textbox(label=\"Prompt for the combine step\", value=COMBINE_TEMPLATE_TXT,\n",
    "                                                        lines=10, max_lines=50, show_copy_button=True, interactive=True)\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=25):\n",
    "                    bttn_summ_mrlc = gr.Button(\"Summarize with Map-Reduce LangChain\", variant='primary')\n",
    "        with gr.Tab(label=\"Map-Reduce Manual\") as tab_mrmn:\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=25):\n",
    "                    map_num_predict = gr.Number(label='Number of tokens to predict at mapping step', minimum=128, step=128, value=config[\"map_num_predict\"], interactive=True)\n",
    "                with gr.Column(scale=1, min_width=25):\n",
    "                    combine_num_predict = gr.Number(label='Number of tokens to predict at refine step', minimum=128, step=128, value=config[\"combine_num_predict\"], interactive=True)\n",
    "                with gr.Column(scale=4):\n",
    "                    with gr.Accordion(label=\"Prompt Templates\", open=False):\n",
    "                        map_prompt_txt_mrmn = gr.Textbox(label=\"Prompt for the mapping step\", value=MAP_TEMPLATE_TXT,\n",
    "                                                        lines=10, max_lines=50, show_copy_button=True)\n",
    "                        combine_prompt_txt_mrmn = gr.Textbox(label=\"Prompt for the combine step\", value=COMBINE_TEMPLATE_TXT,\n",
    "                                                        lines=10, max_lines=50, show_copy_button=True)\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=25):\n",
    "                    bttn_summ_mrmn = gr.Button(\"Summarize with Map-Reduce Manual\", variant='primary')\n",
    "        with gr.Tab(label=\"Refine LangChain\") as tab_rflc:\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=25):\n",
    "                    refine_num_predict = gr.Number(label='Number of tokens to predict', minimum=128, step=128, value=config[\"refine_num_predict\"], interactive=True)\n",
    "                with gr.Column(scale=4):\n",
    "                    with gr.Accordion(label=\"Prompt Templates\", open=False):\n",
    "                        question_prompt_txt_rl = gr.Textbox(label=\"Prompt for the each split doc\", value=QUESTION_TEMPLATE_TXT,\n",
    "                                                        lines=10, max_lines=50, show_copy_button=True, interactive=True)\n",
    "                        refine_prompt_txt_rl = gr.Textbox(label=\"Prompt for the refine step\", value=REFINE_TEMPLATE_TXT,\n",
    "                                                        lines=10, max_lines=50, show_copy_button=True, interactive=True)\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1, min_width=25):\n",
    "                    bttn_summ_rflc = gr.Button(\"Summarize with Refine LangChain\", variant='primary')\n",
    "                    \n",
    "                \n",
    "        gr.Markdown(\"## Summary\")\n",
    "        with gr.Row():\n",
    "            with gr.Column() as r3c2:\n",
    "                    trns_sum = gr.Textbox(label=\"Summary\", show_copy_button=True)\n",
    "\n",
    "    ### events\n",
    "\n",
    "                    \n",
    "    bttn_info_get.click(fn=get_youtube_info,\n",
    "                        inputs=url,\n",
    "                        outputs=[title, desc],\n",
    "                        api_name=\"get_youtube_info\"\n",
    "                        )\n",
    "        \n",
    "    bttn_trns_get.click(fn=get_youtube_transcript,\n",
    "                        inputs=url,\n",
    "                        outputs=[trns_raw, tkncount]\n",
    "                        )\n",
    "\n",
    "    bttn_summ_mrmn.click(fn=get_summary_map_reduce_manual,\n",
    "    inputs=[trns_raw, map_prompt_txt_mrmn, combine_prompt_txt_mrmn],\n",
    "    outputs=trns_sum\n",
    "    )\n",
    "\n",
    "    bttn_summ_mrlc.click(fn=get_summary_map_reduce_langchain,\n",
    "    inputs=[trns_raw, map_prompt_txt_mrlc, combine_prompt_txt_mrlc],\n",
    "    outputs=trns_sum\n",
    "    )\n",
    "\n",
    "\n",
    "    bttn_summ_rflc.click(fn=get_summary_refine_langchain,\n",
    "    inputs=[trns_raw, refine_prompt_txt_rl, question_prompt_txt_rl],\n",
    "    outputs=trns_sum\n",
    "    )\n",
    "    \n",
    "    bttn_clear.add([url, title, desc, trns_raw, trns_sum, tkncount])\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
